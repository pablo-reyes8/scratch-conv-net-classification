{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Overview: `cnn_full`\n",
    "\n",
    "This notebook (`cnn_full.ipynb`) contains a self‑contained implementation of a small convolutional neural network, “from scratch,” organized into the following sections:\n",
    "\n",
    "1. **Imports & Setup**  \n",
    "   - Standard libraries (`numpy`, `pathlib`, `tqdm`, `PIL`, `pandas`)  \n",
    "   - Utility functions for image loading, label encoding, and batch generation  \n",
    "\n",
    "2. **Activation Layers**  \n",
    "   - **ReLU** (if you want more activations there are in src/activations) \n",
    "\n",
    "3. **Convolution & Pooling Layers**  \n",
    "   - **Conv2D**: 2D convolution with He/rand init, padding/stride, Adam updates  \n",
    "   - **MaxPool2D**: 2×2 (or custom) max pooling layer  \n",
    "   - Core routines: `conv_single_step`, `conv_forward`, `conv_backward`, `pool_forward`, `pool_backward`  \n",
    "\n",
    "4. **Dense & Flatten Layers**  \n",
    "   - **Flatten**: reshape from (m, H, W, C) → (m, features)  \n",
    "   - **Dense**: fully connected + softmax output, with Adam updates  \n",
    "\n",
    "5. **Model Builder & Core Passes**  \n",
    "   - `crear_modelo(filters, pool, n_classes, …)`: assemble a sequential layer list  \n",
    "   - `conv_net_forward(layers, X)`: propagate one batch through all layers, cache for backprop  \n",
    "   - `conv_net_backward(layers, A_out, y_batch, lr)`: compute gradients and update trainable layers  \n",
    "\n",
    "6. **Training Loop**  \n",
    "   - `full_cnn(filters, pool, df_train, epochs, batch_size, lr)`:  \n",
    "     - Splits data into batches via `batch_generator`  \n",
    "     - Runs forward/backward passes, Adam updates per batch  \n",
    "     - Tracks epoch‑wise loss & accuracy in a history dict  \n",
    "     - Displays real‑time progress with `tqdm`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3800 filas, Val: 950 filas\n",
      "Test: 794 filas\n"
     ]
    }
   ],
   "source": [
    "base_dir = Path('..') / Path('plant-seedlings-classification')\n",
    "train_dir = base_dir / 'train'\n",
    "test_dir  = base_dir / 'test'\n",
    "\n",
    "## Labels \n",
    "train_rows = []\n",
    "for class_dir in train_dir.iterdir():\n",
    "    if class_dir.is_dir():\n",
    "        for img_path in class_dir.glob('*.*'):  \n",
    "            train_rows.append({\n",
    "                'filepath': str(img_path),\n",
    "                'label': class_dir.name})\n",
    "            \n",
    "df_train = pd.DataFrame(train_rows)\n",
    "df_train_shuffled = df_train.sample(frac=1, random_state=9).reset_index(drop=True)\n",
    "\n",
    "n = len(df_train)\n",
    "split_idx = int(0.8 * n) \n",
    "\n",
    "train_df = df_train.iloc[:split_idx].copy()\n",
    "val_df = df_train.iloc[split_idx:].copy()\n",
    "print(f\"Train: {len(train_df)} filas, Val: {len(val_df)} filas\")\n",
    "\n",
    "\n",
    "## No labels \n",
    "test_rows = []\n",
    "for img_path in test_dir.glob('*.*'):\n",
    "    test_rows.append({'filepath': str(img_path)})\n",
    "df_test = pd.DataFrame(test_rows)\n",
    "\n",
    "print(f'Test: {len(df_test)} filas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_list = sorted(df_train['label'].unique())\n",
    "label2idx  = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "def load_image(path: str, target_size: tuple = (64, 64)):\n",
    "    \"\"\"\n",
    "    Load an image from disk, resize it, and normalize pixel values.\n",
    "\n",
    "    Args:\n",
    "        path (str): Filesystem path to the image.\n",
    "        target_size (tuple of int): Desired output size as (width, height).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: RGB image array of shape (height, width, 3), dtype float32,\n",
    "                    with values scaled to [0.0, 1.0].\n",
    "    \"\"\"\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    try:\n",
    "        resample = Image.Resampling.LANCZOS\n",
    "    except AttributeError:\n",
    "        resample = Image.LANCZOS\n",
    "    img = img.resize(target_size, resample=resample)\n",
    "    return np.array(img, dtype=np.float32) / 255.0\n",
    "\n",
    "\n",
    "def encode_labels(label_batch: list, label2idx: dict, label_list: list):\n",
    "    \"\"\"\n",
    "    Convert a list of label strings to one-hot encoded vectors.\n",
    "\n",
    "    Args:\n",
    "        label_batch (list of str): Labels for the current batch.\n",
    "        label2idx (dict): Mapping from label string to integer index.\n",
    "        label_list (list of str): Full list of possible labels in order.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: One-hot matrix of shape (batch_size, num_classes), dtype float32.\n",
    "    \"\"\"\n",
    "    idxs = [label2idx[label] for label in label_batch]\n",
    "    one_hot = np.zeros((len(idxs), len(label_list)), dtype=np.float32)\n",
    "    one_hot[np.arange(len(idxs)), idxs] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def batch_generator(df, label2idx: dict, label_list: list , batch_size: int = 32, shuffle: bool = True, target_size: tuple = (64, 64)):\n",
    "    \"\"\"\n",
    "    Infinite generator yielding batches of images and one-hot labels.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Must contain columns 'filepath' and 'label'.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        shuffle (bool): Whether to shuffle df at the start of each epoch.\n",
    "        target_size (tuple of int): Size to resize images (width, height).\n",
    "\n",
    "    Yields:\n",
    "        Tuple[np.ndarray, np.ndarray]:\n",
    "            - X: Array of shape (batch_size, H, W, 3), dtype float32.\n",
    "            - y: One-hot labels of shape (batch_size, num_classes), dtype float32.\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            df = df.sample(frac=1).reset_index(drop=True)\n",
    "        for i in range(0, n, batch_size):\n",
    "            batch = df.iloc[i:i + batch_size]\n",
    "            X = np.stack([load_image(fp, target_size) for fp in batch['filepath']])\n",
    "            y = encode_labels(batch['label'].tolist(), label2idx, label_list)\n",
    "            yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 64, 64, 3) (16, 12)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "gen = batch_generator(train_df, batch_size=batch_size, label2idx=label2idx , label_list=label_list)\n",
    "X_batch, y_batch = next(gen)\n",
    "print(X_batch.shape, y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliar functios for Conv2d layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2 * pad, n_W + 2 * pad, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(pad, int) or pad < 0:\n",
    "        raise ValueError(\"`pad` debe ser un entero >= 0.\")\n",
    "    if X.ndim != 4:\n",
    "        raise ValueError(\"`X` debe tener forma (m, n_H, n_W, n_C).\")\n",
    "    \n",
    "    m, n_H, n_W, n_C = X.shape\n",
    "    X_pad = np.zeros((m, n_H + 2*pad,n_W + 2*pad,n_C) , dtype=X.dtype)\n",
    "    X_pad[:, pad:pad+n_H, pad:pad+n_W, :] = X\n",
    "    \n",
    "    return X_pad\n",
    "\n",
    "\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Perform a single convolution step on a slice of the input.\n",
    "\n",
    "    Args:\n",
    "        a_slice_prev (np.ndarray): Input slice of shape (f, f, n_C_prev).\n",
    "        W (np.ndarray): Filter weights of shape (f, f, n_C_prev).\n",
    "        b (np.ndarray or float): Bias term, broadcastable to a scalar.\n",
    "\n",
    "    Returns:\n",
    "        float: The result of applying the filter and bias (i.e., sum(a_slice_prev * W) + b).\n",
    "    \"\"\"\n",
    "\n",
    "    s = a_slice_prev * W\n",
    "    Z = np.sum(s)\n",
    "    Z = Z + b.item()\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass for the Conv Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, \n",
    "        numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "\n",
    "    if not isinstance(hparameters, dict):\n",
    "        raise ValueError(\"`hparameters` debe ser un dict con 'stride' y 'pad'.\")\n",
    "    if 'stride' not in hparameters or 'pad' not in hparameters:\n",
    "        raise KeyError(\"`hparameters` requiere las claves 'stride' y 'pad'.\")\n",
    "    stride, pad = hparameters['stride'], hparameters['pad']\n",
    "    if not (isinstance(stride, int) and stride > 0):\n",
    "        raise ValueError(\"`stride` debe ser un entero > 0.\")\n",
    "    if not (isinstance(pad, int) and pad >= 0):\n",
    "        raise ValueError(\"`pad` debe ser un entero >= 0.\")\n",
    "\n",
    "    n_H = int((n_H_prev - f + 2*pad) / stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2*pad) / stride) + 1\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "\n",
    "    for i in range(m):       # para cada imagen\n",
    "        a_prev_pad = A_prev_pad[i] # shape (n_H_prev+2pad, n_W_prev+2pad, n_C_prev)\n",
    "\n",
    "        for h in range(n_H): # recorre ejes verticales\n",
    "            vert_start = h * stride\n",
    "            vert_end   = vert_start + f \n",
    "            for w in range(n_W):                       # recorre ejes horizontales\n",
    "                horiz_start = w * stride\n",
    "                horiz_end   = horiz_start + f\n",
    "                for c in range(n_C):                   # recorre cada filtro / canal de salida\n",
    "                    a_slice_prev = a_prev_pad[\n",
    "                        vert_start:vert_end,\n",
    "                        horiz_start:horiz_end,:]     # shape (f, f, n_C_prev)\n",
    "                        \n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev,W[:, :, :, c],b[:, :, :, c])\n",
    "                    \n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Forward pass for a 2D pooling layer.\n",
    "\n",
    "    Args:\n",
    "        A_prev (np.ndarray): Input data of shape (m, n_H_prev, n_W_prev, n_C_prev).\n",
    "        hparameters (dict): Dictionary with keys:\n",
    "            - \"f\" (int): size of the pooling window (f × f).\n",
    "            - \"stride\" (int): stride for moving the window.\n",
    "        mode (str): Pooling mode, either \"max\" or \"average\".\n",
    "\n",
    "    Returns:\n",
    "        A (np.ndarray): Output of the pooling layer, shape (m, n_H, n_W, n_C_prev).\n",
    "        cache (tuple): Cached values (A_prev, hparameters) for the backward pass.\n",
    "    \"\"\"\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "\n",
    "\n",
    "    if mode not in (\"max\", \"average\"):\n",
    "        raise ValueError(\"`mode` debe ser 'max' o 'average'\")\n",
    "    if not isinstance(f, int) or f <= 0:\n",
    "        raise ValueError(\"`f` debe ser un entero > 0\")\n",
    "    if not isinstance(stride, int) or stride <= 0:\n",
    "        raise ValueError(\"`stride` debe ser un entero > 0\")\n",
    "    # Asegurarnos de que (n_H_prev - f) es divisible por stride\n",
    "    if (n_H_prev - f) % stride != 0 or (n_W_prev - f) % stride != 0:\n",
    "        raise ValueError(\"Dimensiones inválidas: comprueba f y stride.\")\n",
    "\n",
    "    \n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    for i in range(m):             \n",
    "        for h in range(n_H):       \n",
    "            vert_start = h * stride\n",
    "            vert_end   = vert_start + f\n",
    "            \n",
    "            for w in range(n_W):    \n",
    "                horiz_start = w * stride\n",
    "                horiz_end   = horiz_start + f\n",
    "                \n",
    "                for c in range(n_C): \n",
    "                    a_prev_slice = A_prev[i,vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)      \n",
    "    \n",
    "    cache = (A_prev, hparameters)\n",
    "\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward function for the convolution net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution layer\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"    \n",
    "    A_prev, W, b, hparameters = cache\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    f, f, n_C_prev, n_C  = W.shape\n",
    "    _, n_H, n_W, _= dZ.shape\n",
    "    \n",
    "    dA_prev = np.zeros_like(A_prev , dtype=A_prev.dtype)     \n",
    "    dW  = np.zeros_like(W , dtype=W.dtype)     \n",
    "    db  = np.zeros_like(b , dtype=b.dtype)      \n",
    "    \n",
    "    A_prev_pad   = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad  = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i]  # (n_H_prev+2pad, n_W_prev+2pad, n_C_prev)\n",
    "        da_prev_pad = dA_prev_pad[i] # igual shape que a_prev_pad\n",
    "        \n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    # Encontrar coordenadas del slice\n",
    "                    vert_start  = h * stride\n",
    "                    vert_end    = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end   = horiz_start + f\n",
    "        \n",
    "                    # Extraer slice de A_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:] \n",
    "                    \n",
    "                    # dA_prev_pad: distribuye dZ * W sobre la ventana correspondiente\n",
    "                    da_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:] += W[:, :, :, c] * dZ[i, h, w, c]\n",
    "                    \n",
    "                    # dW: gradiente del filtro c es sum(a_slice * dZ)\n",
    "                    dW[:, :, :, c] += a_slice * dZ[i, h, w, c]\n",
    "                    \n",
    "                    # db: gradiente del bias c es la suma de dZ sobre todos los ejemplos y posiciones\n",
    "                    db[:, :, :, c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        if pad != 0:\n",
    "            dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "        else:\n",
    "            A_prev[i, :, :, :] = da_prev_pad     \n",
    "\n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Create a boolean mask identifying the maximum entry in a 2D window.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): 2D array of shape (f, f).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Boolean mask of the same shape as x, with True at the position(s)\n",
    "                    of the maximum value in x.\n",
    "    \"\"\"  \n",
    "    mask = (x == np.max(x))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Evenly distribute a scalar value over a matrix of specified shape.\n",
    "\n",
    "    Args:\n",
    "        dz (float): Scalar value to distribute.\n",
    "        shape (tuple of int): Tuple (n_H, n_W) specifying the output matrix dimensions.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (n_H, n_W) where each element equals dz / (n_H * n_W).\n",
    "    \"\"\" \n",
    "    (n_H, n_W) = shape\n",
    "    average = dz / (n_H * n_W)\n",
    "\n",
    "    a = np.ones((n_H, n_W)) * average\n",
    "    \n",
    "    return a\n",
    "\n",
    "\n",
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Perform the backward pass for a 2D pooling layer.\n",
    "\n",
    "    Args:\n",
    "        dA (np.ndarray): Gradient of the cost with respect to the output of the pooling layer,\n",
    "                         of shape (m, n_H, n_W, n_C).\n",
    "        cache (tuple): Tuple containing:\n",
    "            - A_prev (np.ndarray): Input data to the pooling layer during forward pass,\n",
    "                                    shape (m, n_H_prev, n_W_prev, n_C_prev).\n",
    "            - hparameters (dict): Dictionary with keys:\n",
    "                'stride' (int): Stride used in pooling,\n",
    "                'f' (int): Size of the pooling window.\n",
    "        mode (str): Pooling mode, either 'max' or 'average'.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Gradient of the cost with respect to the input of the pooling layer,\n",
    "                    of shape (m, n_H_prev, n_W_prev, n_C_prev).\n",
    "    \"\"\"\n",
    "\n",
    "    A_prev, hparameters = cache\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (_, n_H, n_W, n_C) = dA.shape\n",
    "    \n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "    \n",
    "    for i in range(m):\n",
    "\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    " \n",
    "                    vert_start  = h * stride\n",
    "                    vert_end    = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end   = horiz_start + f\n",
    "\n",
    "                    if mode == \"max\":\n",
    " \n",
    "                        a_prev_slice = a_prev[vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "\n",
    "                        dA_prev[i,vert_start:vert_end,horiz_start:horiz_end,c] += mask * dA[i, h, w, c]\n",
    "\n",
    "                    elif mode == \"average\":\n",
    "                        a = distribute_value(dA[i, h, w, c], (f, f))\n",
    "                        dA_prev[i,vert_start:vert_end,horiz_start:horiz_end,c] += a\n",
    "\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the clases for the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    \"\"\"\n",
    "    2D convolutional layer with built‑in Adam optimizer support.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_C_prev, n_C,\n",
    "        f, stride = 1, pad = 0, initialization = 'he',scale = 0.01, seed = None):\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize a Conv2D layer.\n",
    "\n",
    "        Args:\n",
    "            n_C_prev (int): Number of channels in the input (depth of A_prev).\n",
    "            n_C (int): Number of filters (output channels).\n",
    "            f (int): Size of each filter (filters are f x f).\n",
    "            stride (int, optional): Stride length for the convolution. Defaults to 1.\n",
    "            pad (int, optional): Number of zero-padding pixels around the input. Defaults to 0.\n",
    "            initialization (str, optional): Weight init method: 'he' or 'rand'. Defaults to 'he'.\n",
    "            scale (float, optional): Scaling factor for 'rand' init. Defaults to 0.01.\n",
    "            seed (int or None, optional): Random seed for reproducibility. Defaults to None.\n",
    "\n",
    "        Attributes:\n",
    "            W (np.ndarray): Filters of shape (f, f, n_C_prev, n_C).\n",
    "            b (np.ndarray): Biases of shape (1, 1, 1, n_C).\n",
    "            stride (int): Convolution stride.\n",
    "            pad (int): Padding size.\n",
    "            cache (tuple): Cached values for backward pass.\n",
    "            dW (np.ndarray): Gradient of W.\n",
    "            db (np.ndarray): Gradient of b.\n",
    "            mW, vW, mb, vb (np.ndarray): Adam first/second moment buffers.\n",
    "            t (int): Adam timestep counter.\n",
    "        \"\"\"\n",
    "         \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.n_C_prev = n_C_prev\n",
    "        self.n_C      = n_C\n",
    "        self.f        = f\n",
    "        self.stride   = stride\n",
    "        self.pad      = pad\n",
    "        self.name     = 'conv2d'\n",
    "\n",
    "        if initialization.lower() == 'he':\n",
    "            factor = np.sqrt(2.0 / (f * f * n_C_prev))\n",
    "            self.W = np.random.randn(f, f, n_C_prev, n_C) * factor\n",
    "        elif initialization.lower() == 'rand':\n",
    "            self.W = np.random.randn(f, f, n_C_prev, n_C) * scale\n",
    "        else:\n",
    "            raise ValueError(\"`initialization` debe ser 'rand' o 'he'\")\n",
    "        self.b = np.zeros((1, 1, 1, n_C), dtype=self.W.dtype)\n",
    "\n",
    "\n",
    "        self.cache = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.vW = np.zeros_like(self.W)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.vb = np.zeros_like(self.b)\n",
    "        self.t  = 0\n",
    "\n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the convolution.\n",
    "\n",
    "        Args:\n",
    "            A_prev (np.ndarray): Input data of shape (m, n_H_prev, n_W_prev, n_C_prev).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Convolved output Z of shape (m, n_H, n_W, n_C).\n",
    "        \"\"\"\n",
    "        hparams = {'stride': self.stride, 'pad': self.pad}\n",
    "        Z, cache = conv_forward(A_prev, self.W, self.b, hparams)\n",
    "        self.cache = cache\n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Perform the backward pass of the convolution.\n",
    "\n",
    "        Args:\n",
    "            dZ (np.ndarray): Gradient of the loss with respect to the output Z.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Gradient with respect to the input A_prev.\n",
    "        \"\"\"\n",
    "        dA_prev, dW, db = conv_backward(dZ, self.cache)\n",
    "        self.dW, self.db = dW, db\n",
    "        return dA_prev\n",
    "\n",
    "    def update_adam(self, lr, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        \"\"\"\n",
    "        Update parameters using Adam optimization. Call after backward().\n",
    "\n",
    "        Args:\n",
    "            lr (float): Learning rate.\n",
    "            beta1 (float, optional): Exponential decay rate for the first moment. Defaults to 0.9.\n",
    "            beta2 (float, optional): Exponential decay rate for the second moment. Defaults to 0.999.\n",
    "            eps (float, optional): Small constant to prevent division by zero. Defaults to 1e-8.\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "\n",
    "        self.mW = beta1 * self.mW + (1 - beta1) * self.dW\n",
    "        self.mb = beta1 * self.mb + (1 - beta1) * self.db\n",
    "\n",
    "\n",
    "        self.vW = beta2 * self.vW + (1 - beta2) * (self.dW ** 2)\n",
    "        self.vb = beta2 * self.vb + (1 - beta2) * (self.db ** 2)\n",
    "\n",
    "        mW_hat = self.mW / (1 - beta1 ** self.t)\n",
    "        mb_hat = self.mb / (1 - beta1 ** self.t)\n",
    "        vW_hat = self.vW / (1 - beta2 ** self.t)\n",
    "        vb_hat = self.vb / (1 - beta2 ** self.t)\n",
    "\n",
    "        self.W -= lr * mW_hat / (np.sqrt(vW_hat) + eps)\n",
    "        self.b -= lr * mb_hat / (np.sqrt(vb_hat) + eps)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MaxPool2D:\n",
    "    \"\"\"\n",
    "    2D max pooling layer.\n",
    "\n",
    "    Performs non-overlapping max pooling over input feature maps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, f: int, stride: int = 2):\n",
    "        \"\"\"\n",
    "        Initialize the MaxPool2D layer.\n",
    "\n",
    "        Args:\n",
    "            f (int): Size of the pooling window (f × f).\n",
    "            stride (int): Stride (step) size for both height and width.\n",
    "        Raises:\n",
    "            ValueError: If f or stride is not a positive integer.\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(f, int) or f <= 0:\n",
    "            raise ValueError(\"`f` debe ser un entero > 0.\")\n",
    "        if not isinstance(stride, int) or stride <= 0:\n",
    "            raise ValueError(\"`stride` debe ser un entero > 0.\")\n",
    "\n",
    "        self.f = f\n",
    "        self.stride = stride\n",
    "        self.cache = None\n",
    "        self.name = 'poollayer'\n",
    "\n",
    "    def forward(self, A_prev: np.ndarray):\n",
    "        \"\"\"\n",
    "        Forward pass for max pooling.\n",
    "\n",
    "        Args:\n",
    "            A_prev (np.ndarray): Input data of shape (m, H_prev, W_prev, C_prev).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Pooled output of shape (m, H, W, C_prev),\n",
    "                        where H and W depend on f and stride.\n",
    "        \"\"\"\n",
    "\n",
    "        hparams = {'f': self.f, 'stride': self.stride}\n",
    "        A, cache = pool_forward(A_prev, hparams, mode='max')\n",
    "        self.cache = cache\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward pass for max pooling.\n",
    "\n",
    "        Args:\n",
    "            dA (np.ndarray): Gradient of the loss with respect to the pooled output,\n",
    "                             of shape (m, H, W, C_prev).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Gradient with respect to the input A_prev,\n",
    "                        of shape (m, H_prev, W_prev, C_prev).\n",
    "        \"\"\"\n",
    "\n",
    "        A_prev, hparams = self.cache\n",
    "        dA_prev = pool_backward(dA, (A_prev, hparams), mode='max')\n",
    "        return dA_prev\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit (ReLU) activation layer.\n",
    "\n",
    "    Attributes:\n",
    "        name (str): identifier for this activation.\n",
    "        cache (np.ndarray): stores input Z for use in backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the ReLU layer.\n",
    "        \"\"\"\n",
    "        self.name = 'relu'\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, Z):\n",
    "        \"\"\"\n",
    "        Forward pass of ReLU.\n",
    "\n",
    "        Args:\n",
    "            Z (np.ndarray): pre-activation input of any shape.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: activations A, where A = max(0, Z).\n",
    "        \"\"\"\n",
    "        A = np.maximum(0, Z)\n",
    "        self.cache = Z\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward pass of ReLU.\n",
    "\n",
    "        Args:\n",
    "            dA (np.ndarray): gradient of the loss with respect to the activation output A.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: gradient of the loss with respect to Z.\n",
    "        \"\"\"\n",
    "        Z = self.cache\n",
    "        dZ = dA.copy()\n",
    "        dZ[Z <= 0] = 0\n",
    "        return dZ\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Flatten:\n",
    "    \"\"\"\n",
    "    Flatten layer that reshapes its input into a 2D array\n",
    "    (batch_size, features).\n",
    "\n",
    "    Attributes:\n",
    "        name (str): Identifier for this layer.\n",
    "        cache (tuple): Stores the original input shape for backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the Flatten layer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.name = 'flatten'\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Forward pass: flatten the input.\n",
    "\n",
    "        Args:\n",
    "            A_prev (np.ndarray): Input array of shape (m, ...),\n",
    "                                 where m is the batch size and ... \n",
    "                                 represents any number of additional dimensions.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Flattened output of shape (m, features),\n",
    "                        where features = product of the ... dimensions.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.cache = A_prev.shape\n",
    "        m = A_prev.shape[0]\n",
    "        return A_prev.reshape(m, -1)\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward pass: reshape upstream gradients to the original input shape.\n",
    "\n",
    "        Args:\n",
    "            dA (np.ndarray): Gradient of the loss w.r.t. the flattened output,\n",
    "                             of shape (m, features).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Gradient reshaped to the original input shape stored in cache.\n",
    "        \"\"\"\"\"\"\n",
    "        Backward pass: reshape upstream gradients to the original input shape.\n",
    "\n",
    "        Args:\n",
    "            dA (np.ndarray): Gradient of the loss w.r.t. the flattened output,\n",
    "                             of shape (m, features).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Gradient reshaped to the original input shape stored in cache.\n",
    "        \"\"\"\n",
    "\n",
    "        return dA.reshape(self.cache)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Dense:\n",
    "    \"\"\"\n",
    "    Fully‑connected (dense) layer with softmax activation and Adam optimizer support.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_units, initialization='rand', scale=0.01, seed=None):\n",
    "        \"\"\"\n",
    "        Initialize the Dense layer (weights and Adam buffers will be set on first forward).\n",
    "\n",
    "        Args:\n",
    "            n_units (int): Number of output neurons.\n",
    "            initialization (str): 'he' for He initialization or 'rand' for scaled random.\n",
    "            scale (float): Scale for 'rand' initialization.\n",
    "            seed (int or None): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_inputs = None\n",
    "        self.n_units = n_units\n",
    "        self.initialization = initialization\n",
    "        self.scale = scale\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        \n",
    "\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        self.mW = None\n",
    "        self.vW = None\n",
    "        self.mb = None\n",
    "        self.vb = None\n",
    "        self.t = 0\n",
    "        \n",
    "        self.cache = None\n",
    "        self.name = 'dense'\n",
    "\n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Forward pass: linear transform followed by softmax.\n",
    "\n",
    "        Args:\n",
    "            A_prev (np.ndarray): Input data of shape (m, features).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Output probabilities of shape (m, n_units).\n",
    "        \"\"\"\n",
    "\n",
    "        m, features = A_prev.shape\n",
    "\n",
    "        if self.W is None:\n",
    "            if self.seed is not None:\n",
    "                np.random.seed(self.seed)\n",
    "            self.n_inputs = features\n",
    "\n",
    "            if self.initialization.lower() == 'he':\n",
    "                factor = np.sqrt(2.0 / self.n_inputs)\n",
    "                self.W = np.random.randn(self.n_inputs, self.n_units) * factor\n",
    "            else:\n",
    "                self.W = np.random.randn(self.n_inputs, self.n_units) * self.scale\n",
    "            self.b = np.zeros((1, self.n_units), dtype=self.W.dtype)\n",
    "\n",
    "            self.mW = np.zeros_like(self.W)\n",
    "            self.vW = np.zeros_like(self.W)\n",
    "            self.mb = np.zeros_like(self.b)\n",
    "            self.vb = np.zeros_like(self.b)\n",
    "\n",
    "        Z = A_prev @ self.W + self.b\n",
    "        exps = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "        A = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        self.cache = (A_prev, A)\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weights, biases, and inputs.\n",
    "\n",
    "        Args:\n",
    "            dA (np.ndarray): Upstream gradient of shape (m, n_units).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Gradient w.r.t. input A_prev, shape (m, n_inputs).\n",
    "        \"\"\"\n",
    "\n",
    "        A_prev, A = self.cache\n",
    "        m = A_prev.shape[0]\n",
    "\n",
    "        S = dA * A\n",
    "        sum_S = np.sum(S, axis=1, keepdims=True)\n",
    "        dZ = S - A * sum_S\n",
    "\n",
    "        self.dW = (A_prev.T @ dZ) / m\n",
    "        self.db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        dA_prev = dZ @ self.W.T\n",
    "        return dA_prev\n",
    "\n",
    "    def update_adam(self, lr, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        \"\"\"\n",
    "        Update parameters W and b using Adam optimizer.\n",
    "\n",
    "        Must be called after backward().\n",
    "\n",
    "        Args:\n",
    "            lr (float): Learning rate.\n",
    "            beta1 (float): Exponential decay rate for first moment.\n",
    "            beta2 (float): Exponential decay rate for second moment.\n",
    "            eps (float): Small epsilon to avoid division by zero.\n",
    "        \"\"\"\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        # Update biased first moment estimates\n",
    "        self.mW = beta1 * self.mW + (1 - beta1) * self.dW\n",
    "        self.mb = beta1 * self.mb + (1 - beta1) * self.db\n",
    "\n",
    "        # Update biased second moment estimates\n",
    "        self.vW = beta2 * self.vW + (1 - beta2) * (self.dW ** 2)\n",
    "        self.vb = beta2 * self.vb + (1 - beta2) * (self.db ** 2)\n",
    "\n",
    "        # Compute bias-corrected moments\n",
    "        mW_hat = self.mW / (1 - beta1 ** self.t)\n",
    "        mb_hat = self.mb / (1 - beta1 ** self.t)\n",
    "        vW_hat = self.vW / (1 - beta2 ** self.t)\n",
    "        vb_hat = self.vb / (1 - beta2 ** self.t)\n",
    "\n",
    "        # Parameter update\n",
    "        self.W -= lr * mW_hat / (np.sqrt(vW_hat) + eps)\n",
    "        self.b -= lr * mb_hat / (np.sqrt(vb_hat) + eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute cost for clasification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cost(A_final , labels , tipe ='CrossEntropy' ):\n",
    "    \"\"\"Compute the loss between predicted probabilities and true labels, with optional L2 regularization.\n",
    "\n",
    "    Supports binary cross-entropy for two-class problems or categorical\n",
    "    cross-entropy for multi-class problems. \n",
    "\n",
    "    Args:\n",
    "        A_final (np.ndarray): Predicted probabilities, shape (n_y, m).\n",
    "        labels (np.ndarray): True labels, shape (n_y, m) or (m,); will be\n",
    "            reshaped to (1, m) if necessary.\n",
    "        tipe (str): Type of cost to compute:\n",
    "            - 'BinaryCrossEntropy': binary cross-entropy loss.\n",
    "            - 'CrossEntropy': categorical cross-entropy loss.\n",
    "        caches (dict, optional): Dictionary of cached values from forward pass.\n",
    "            Used to extract weight matrices 'W1', 'W2', … when `regularization=True`.\n",
    "\n",
    "    Returns:\n",
    "        float: The scalar loss value (including regularization term if enabled).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `tipe` is not one of the supported cost types.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure labels are shape (n_y, m)\n",
    "    if labels.ndim == 1:\n",
    "        labels = labels.reshape(1, -1)\n",
    "\n",
    "    m = labels.shape[1]\n",
    "    eps = 1e-15\n",
    "    A_safe = np.clip(A_final, eps, 1 - eps)\n",
    "\n",
    "    # Compute base cost\n",
    "    if tipe == 'BinaryCrossEntropy':\n",
    "        logprobs = (labels * np.log(A_safe) +\n",
    "                    (1 - labels) * np.log(1 - A_safe))\n",
    "        cost = - (1 / m) * np.sum(logprobs)\n",
    "\n",
    "    elif tipe == 'CrossEntropy':\n",
    "        logprobs = labels * np.log(A_safe)\n",
    "        cost = - (1 / m) * np.sum(logprobs)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"`tipe` must be 'BinaryCrossEntropy' or 'CrossEntropy'\")\n",
    "\n",
    "    return float(np.squeeze(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backward pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_net_forward(layers, X):\n",
    "    \"\"\"\n",
    "    Perform forward propagation through a sequence of convolutional network layers.\n",
    "\n",
    "    Args:\n",
    "        layers (list): Ordered list of layer objects with .name and .forward():\n",
    "            - 'conv2d': convolutional layer, caches (A_prev, W, b, hparams)\n",
    "            - 'poollayer': max‑pooling layer, caches (A_prev, hparams)\n",
    "            - 'relu': ReLU activation, caches Z\n",
    "            - 'dense': fully‑connected + softmax, caches (A_prev_flat, A)\n",
    "        X (np.ndarray): Input batch of shape (m, H, W, C) for conv layers.\n",
    "\n",
    "    Returns:\n",
    "        A (np.ndarray): Output activations from the final layer.\n",
    "        caches (dict): Mapping string keys to cached arrays needed for backprop.\n",
    "    \"\"\"\n",
    "    caches = {}\n",
    "    A = X\n",
    "    conv, pool, activation, dense = 1, 1, 1, 1\n",
    "\n",
    "    for idx, layer in enumerate(layers):\n",
    "        A_prev = A\n",
    "\n",
    "        if layer.name == 'dense':\n",
    "            m = A_prev.shape[0]\n",
    "            A_prev_flat = A_prev.reshape(m, -1)\n",
    "            A = layer.forward(A_prev_flat)\n",
    "\n",
    "            A_prev_c, A_c = layer.cache\n",
    "            caches['A_prev' + str(dense) + ' dense'] = A_prev_c\n",
    "            caches['A' + str(dense) + ' dense'] = A_c\n",
    "            dense += 1\n",
    "\n",
    "        else:\n",
    "            A = layer.forward(A_prev)\n",
    "\n",
    "            if layer.name == 'conv2d':\n",
    "                A_prev_c, W_c, b_c, hp_c = layer.cache\n",
    "                caches['A_prev' + str(conv) + ' conv2d'] = A_prev_c\n",
    "                caches['W'+ str(conv) + ' conv2d'] = W_c\n",
    "                caches['b'+ str(conv) + ' conv2d'] = b_c\n",
    "                caches['hparameters' + str(conv) + ' conv2d'] = hp_c\n",
    "                conv += 1\n",
    "\n",
    "            elif layer.name == 'poollayer':\n",
    "                A_prev_c, hp_c = layer.cache\n",
    "                caches['A_prev'+ str(pool) + ' PoolLayer'] = A_prev_c\n",
    "                caches['hparameters' + str(pool) + ' PoolLayer'] = hp_c\n",
    "                pool += 1\n",
    "\n",
    "            elif layer.name == 'relu':\n",
    "                Z_c = layer.cache\n",
    "                caches['Z' + str(activation) + ' ReLu'] = Z_c\n",
    "                activation += 1\n",
    "\n",
    "    return A, caches\n",
    "\n",
    "\n",
    "def conv_net_backward(layers , A_out , y_batch , lr):\n",
    "    \"\"\"\n",
    "    Perform the backward pass through a convolutional network and update trainable parameters.\n",
    "\n",
    "    Args:\n",
    "        layers (list): List of layer objects in forward order.\n",
    "        A_out (np.ndarray): Softmax output from the network, shape (m, n_classes).\n",
    "        y_batch (np.ndarray): One-hot encoded true labels, shape (m, n_classes).\n",
    "        lr (float): Learning rate for Adam updates on Conv2D and Dense layers.\n",
    "\n",
    "    Returns:\n",
    "        dict: Gradients for each layer, keyed by layer index and type, useful for debugging.\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    dA_l = A_out - y_batch\n",
    "    conv, pool, activation, dense = 1, 1, 1, 1\n",
    "\n",
    "    for layer in reversed(layers):\n",
    "        dA_post = dA_l\n",
    "\n",
    "        if layer.name == 'dense':\n",
    "            dA_l = layer.backward(dA_post)\n",
    "\n",
    "            grads['dW' + str(dense) + ' dense'] = layer.dW\n",
    "            grads['db' + str(dense) + ' dense'] = layer.db \n",
    "            layer.update_adam(lr)\n",
    "            dense += 1\n",
    "\n",
    "        elif layer.name == 'flatten':\n",
    "            dA_l = layer.backward(dA_post)\n",
    "\n",
    "            grads['dA' + str(dense) + ' flatten'] = dA_l\n",
    "        \n",
    "        elif layer.name == 'poollayer':\n",
    "            dA_l = layer.backward(dA_post)\n",
    "\n",
    "            grads['dA' + str(pool) + ' PoolLayer'] = dA_l\n",
    "            pool += 1\n",
    "\n",
    "        elif layer.name == 'relu':\n",
    "            dA_l = layer.backward(dA_post)\n",
    "\n",
    "            grads['dZ' + str(activation) + ' ReLu'] = dA_l\n",
    "            activation += 1\n",
    "\n",
    "        elif layer.name == 'conv2d':\n",
    "            dA_l = layer.backward(dA_post)\n",
    "\n",
    "            \n",
    "            grads['dW' + str(conv) + ' conv2d'] = layer.dW\n",
    "            grads['db' + str(conv) + ' conv2d'] = layer.db \n",
    "            layer.update_adam(lr)\n",
    "            conv += 1\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Capa desconocida: {layer.name}\") \n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to build and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def crear_modelo(filters, pool, n_classes,\n",
    "                 filter_size=3, stride=1, pad=1,\n",
    "                 pool_filter=2, pool_stride=2, poo='half'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Build a convolutional neural network given layer specifications.\n",
    "\n",
    "    Args:\n",
    "        filters (list of int): Channel counts for each Conv2D layer, e.g. [3, 8, 16].\n",
    "        pool (list of int): Pool indicators per conv block; if pool[i] != 0, add pooling after block i.\n",
    "        n_classes (int): Number of output units in the final Dense layer.\n",
    "        filter_size (int, optional): Height/width of each Conv2D filter. Default is 3.\n",
    "        stride (int, optional): Stride for each Conv2D layer. Default is 1.\n",
    "        pad (int, optional): Zero‑padding for each Conv2D layer. Default is 1.\n",
    "        pool_filter (int, optional): Filter size for MaxPool2D when poo!='half'. Default is 2.\n",
    "        pool_stride (int, optional): Stride for MaxPool2D when poo!='half'. Default is 2.\n",
    "        poo (str, optional): Pooling mode. If 'half', uses (2,2) filter+stride; otherwise uses pool_filter and pool_stride. Default is 'half'.\n",
    "\n",
    "    Returns:\n",
    "        list: Sequence of layer objects [Conv2D, ReLU, (MaxPool2D)..., Flatten, Dense].\n",
    "    \"\"\"\n",
    "    \n",
    "    model = []\n",
    "    for i in range(len(filters)-1):\n",
    "        model.append(Conv2D(filters[i], filters[i+1], f=filter_size,\n",
    "                             stride=stride, pad=pad))\n",
    "        model.append(ReLU())\n",
    "        if pool[i] != 0:\n",
    "            if poo == 'half':\n",
    "                model.append(MaxPool2D(f=2, stride=2))\n",
    "            else:\n",
    "                model.append(MaxPool2D(f=pool_filter, stride=pool_stride))\n",
    "    model.append(Flatten())\n",
    "    model.append(Dense(n_units=n_classes, initialization='he'))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def full_cnn(filters , pool , df_train , epochs ,batch_size , lr):\n",
    "    \"\"\"\n",
    "    Train a CNN end‑to‑end on image data using a simple training loop.\n",
    "\n",
    "    Args:\n",
    "        filters (list[int]): Number of channels for each Conv2D layer, e.g. [3, 8, 16].\n",
    "        pool (list[int]): Indicators for adding a pooling layer after each conv block.\n",
    "        df_train (pd.DataFrame): DataFrame with columns 'filepath' and 'label' for training.\n",
    "        epochs (int): Number of training epochs (full passes over the dataset).\n",
    "        batch_size (int): Number of samples per gradient update.\n",
    "        lr (float): Learning rate for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            model (list): List of layer objects (Conv2D, ReLU, MaxPool2D, Flatten, Dense).\n",
    "            history (dict): Contains two lists:\n",
    "                - 'cost': average cross‑entropy per epoch.\n",
    "                - 'acc':  average accuracy per epoch.\n",
    "    \"\"\"\n",
    "    steps_per_epoch = len(df_train) // batch_size\n",
    "    gen = batch_generator(df_train, batch_size=batch_size)\n",
    "    model = crear_modelo(filters, pool)\n",
    "\n",
    "    history = {'cost': [], 'acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "         \n",
    "        epoch_cost = 0.0\n",
    "        epoch_acc = 0.0\n",
    "\n",
    "        pbar = tqdm(range(steps_per_epoch),  desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "\n",
    "        for _ in pbar:\n",
    "\n",
    "            X_batch, y_batch = next(gen)\n",
    "            A_out, _ = conv_net_forward(model, X_batch)\n",
    "            cost_step = compute_cost(A_out , y_batch , 'CrossEntropy')\n",
    "\n",
    "            preds = np.argmax(A_out, axis=1)\n",
    "            trues = np.argmax(y_batch, axis=1)\n",
    "            acc_step = np.mean(preds == trues)\n",
    "\n",
    "            epoch_cost += cost_step\n",
    "            epoch_acc  += acc_step\n",
    "\n",
    "            conv_net_backward(model , A_out , y_batch , lr)\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'cost': f\"{cost_step:.4f}\",\n",
    "                'acc':  f\"{acc_step:.4f}\"})\n",
    "            \n",
    "        cost_avg = epoch_cost / steps_per_epoch\n",
    "        acc_avg  = epoch_acc  / steps_per_epoch\n",
    "        history['cost'].append(cost_avg)\n",
    "        history['acc'].append(acc_avg)\n",
    "\n",
    "        tqdm.write(f\"→ Epoch {epoch+1}/{epochs} \"f\"— cost_avg: {cost_avg:.4f}, acc_avg: {acc_avg:.4f}\")\n",
    "\n",
    "    return model , history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and tetst model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_batch shape: (2, 12)\n",
      "Sample label indices: [5 9]\n",
      "Sample one‑hot rows:\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "Output shape: (2, 12)\n",
      "Keys in caches: ['A_prev1 conv2d', 'W1 conv2d', 'b1 conv2d', 'hparameters1 conv2d', 'Z1 ReLu', 'A_prev1 PoolLayer', 'hparameters1 PoolLayer', 'A_prev2 conv2d', 'W2 conv2d', 'b2 conv2d', 'hparameters2 conv2d', 'Z2 ReLu', 'A_prev2 PoolLayer', 'hparameters2 PoolLayer', 'A_prev1 dense', 'A1 dense']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = [\n",
    "  Conv2D(n_C_prev=3, n_C=8,  f=3, stride=1, pad=1),\n",
    "  ReLU(),\n",
    "  MaxPool2D(f=2, stride=2),\n",
    "\n",
    "  Conv2D(n_C_prev=8, n_C=16, f=3, stride=1, pad=1),\n",
    "  ReLU(),\n",
    "  MaxPool2D(f=2, stride=2),\n",
    "  \n",
    "  Flatten(),\n",
    "  Dense(n_units=12, initialization='he')]\n",
    "\n",
    "\n",
    "X_batch = np.random.randn(2, 8, 8, 3).astype(np.float32)\n",
    "batch_size  = X_batch.shape[0]\n",
    "num_classes = 12\n",
    "\n",
    "y_int = np.random.randint(0, num_classes, size=batch_size)\n",
    "y_batch = np.zeros((batch_size, num_classes), dtype=np.float32)\n",
    "y_batch[np.arange(batch_size), y_int] = 1.0\n",
    "\n",
    "print(\"y_batch shape:\", y_batch.shape)\n",
    "print(\"Sample label indices:\", y_int[:5])\n",
    "print(\"Sample one‑hot rows:\\n\", y_batch[:5])\n",
    "\n",
    "A_out, all_caches = conv_net_forward(model, X_batch)\n",
    "\n",
    "print(\"Output shape:\", A_out.shape)\n",
    "print(\"Keys in caches:\", list(all_caches.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8494814124497543"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost(A_out , y_batch , 'CrossEntropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-1.08535635e+00,  8.84865820e-01, -2.21738863e+00],\n",
       "         [-4.36401099e-01, -1.50021946e+00, -1.71337533e+00],\n",
       "         [-1.85090685e+00,  1.30147055e-01, -3.37578982e-01],\n",
       "         [ 1.53737918e-01,  1.02734935e+00, -1.18749750e+00],\n",
       "         [ 4.15060997e-01,  6.46146297e-01,  1.33408284e+00],\n",
       "         [ 2.18879044e-01,  5.83041191e-01, -3.27038527e-01],\n",
       "         [-1.08417891e-01,  9.85066772e-01,  8.67435873e-01],\n",
       "         [-2.60802007e+00, -8.33135784e-01,  8.35995793e-01]],\n",
       "\n",
       "        [[ 1.67270911e+00, -1.30358076e+00, -4.56510335e-01],\n",
       "         [-1.70925990e-01,  3.69940192e-01, -9.68445241e-01],\n",
       "         [ 2.79843032e-01,  7.24299312e-01,  1.09825361e+00],\n",
       "         [-1.09861255e+00, -5.84066212e-01,  6.63183987e-01],\n",
       "         [-1.68721192e-02,  1.98960721e-01, -1.26223409e+00],\n",
       "         [-7.63742626e-01,  5.46435237e-01, -6.78651690e-01],\n",
       "         [ 9.95025277e-01,  1.65827310e+00,  6.72506750e-01],\n",
       "         [-3.98395121e-01,  3.90984893e-01,  3.19868493e+00]],\n",
       "\n",
       "        [[-1.40511119e+00, -1.78986937e-01, -3.13783717e+00],\n",
       "         [ 9.34445083e-01,  8.21923912e-01, -2.24317387e-01],\n",
       "         [ 4.38590348e-01, -2.05892753e-02,  6.33640289e-01],\n",
       "         [ 2.07569599e+00,  2.13313371e-01, -4.69323359e-02],\n",
       "         [-1.44725895e+00, -4.84566502e-02,  2.10897422e+00],\n",
       "         [-6.82507277e-01, -3.03634237e-02, -1.94482759e-01],\n",
       "         [ 2.83199936e-01, -1.04803133e+00, -2.18857265e+00],\n",
       "         [ 3.61690283e-01,  3.81470122e-03, -7.63470888e-01]],\n",
       "\n",
       "        [[ 9.62724909e-03,  1.11680605e-01,  1.08942993e-01],\n",
       "         [ 1.15213826e-01,  7.22927034e-01, -1.96921730e+00],\n",
       "         [ 2.57185102e-01, -5.73413633e-03,  2.53275108e+00],\n",
       "         [ 7.83535317e-02,  1.57524541e-01,  2.15205476e-01],\n",
       "         [ 1.31385767e+00,  3.62090081e-01,  1.71583593e+00],\n",
       "         [-1.34700251e+00, -8.53696242e-02, -7.13641286e-01],\n",
       "         [-2.51533478e-01, -1.33777440e-01, -1.39881778e+00],\n",
       "         [-3.83779705e-01, -1.36103618e+00, -7.72550941e-01]],\n",
       "\n",
       "        [[ 3.32541108e-01, -5.66821694e-01, -3.13525349e-01],\n",
       "         [ 6.47342801e-01,  7.63647974e-01,  2.03287768e+00],\n",
       "         [-7.10216224e-01,  1.66262054e+00,  6.53527796e-01],\n",
       "         [-3.20688748e+00, -1.13314606e-01,  3.09248595e-03],\n",
       "         [ 2.95659065e-01,  4.98626083e-01, -5.51612258e-01],\n",
       "         [-1.30723190e+00,  7.54991770e-01,  1.18692005e+00],\n",
       "         [-7.24555552e-01,  7.11233377e-01, -6.61054671e-01],\n",
       "         [-3.32025856e-01, -5.43878138e-01,  6.00749180e-02]],\n",
       "\n",
       "        [[-1.35010958e+00, -6.95237577e-01, -4.57546204e-01],\n",
       "         [-7.38506138e-01, -4.98666227e-01, -1.02832842e+00],\n",
       "         [ 3.48450512e-01,  8.70736659e-01, -2.41469359e+00],\n",
       "         [-1.09493339e+00, -1.31145370e+00,  1.54368258e+00],\n",
       "         [ 9.40069035e-02,  1.80812335e+00,  1.81167200e-01],\n",
       "         [-3.72924805e-01, -2.24768162e-01,  1.18927136e-01],\n",
       "         [-8.57936680e-01,  1.34193766e+00, -1.06242573e+00],\n",
       "         [ 3.89740765e-02, -2.84142995e+00,  8.94541681e-01]],\n",
       "\n",
       "        [[-5.36576867e-01, -9.34683681e-01, -3.76152664e-01],\n",
       "         [ 1.66375470e+00,  2.85953671e-01, -4.14459199e-01],\n",
       "         [-1.02453148e+00,  6.27493083e-01, -5.58337629e-01],\n",
       "         [-6.66719377e-01, -7.31366396e-01, -1.67283162e-01],\n",
       "         [-4.85868782e-01,  9.27135825e-01, -2.25714660e+00],\n",
       "         [ 4.95494097e-01,  1.41732454e+00,  2.98188567e-01],\n",
       "         [-1.34042430e+00,  1.18674886e+00,  2.59741354e+00],\n",
       "         [-9.01305377e-01,  5.80601245e-02, -1.78721443e-01]],\n",
       "\n",
       "        [[-2.59011179e-01, -6.26842141e-01,  1.29866397e+00],\n",
       "         [ 2.69044220e-01, -6.72493458e-01, -2.71772504e-01],\n",
       "         [ 1.62927604e+00,  2.23062181e+00, -8.76231968e-01],\n",
       "         [-4.25499648e-01, -8.33260000e-01, -1.03278220e+00],\n",
       "         [-1.14173017e-01, -6.06462121e-01,  4.36778516e-01],\n",
       "         [ 9.61843729e-01, -2.89241850e-01, -1.15134060e+00],\n",
       "         [ 8.00635636e-01, -7.49125183e-01, -1.08329308e+00],\n",
       "         [-1.36257994e+00, -2.35665888e-01,  4.61941659e-01]]],\n",
       "\n",
       "\n",
       "       [[[ 9.64596197e-02, -8.39564860e-01, -4.51967567e-01],\n",
       "         [-1.57148111e+00,  1.39120555e+00, -7.15068206e-02],\n",
       "         [-1.33411646e+00, -2.27076221e+00,  1.30002081e-01],\n",
       "         [-1.06896555e+00, -1.19873929e+00, -7.37908423e-01],\n",
       "         [ 1.45676732e-01, -2.56216209e-02, -1.18178718e-01],\n",
       "         [ 6.91193223e-01, -1.45495594e-01,  2.81081140e-01],\n",
       "         [ 3.37630920e-02, -1.51301074e+00, -3.00774992e-01],\n",
       "         [ 1.56880987e+00, -5.01116216e-01, -6.80676162e-01]],\n",
       "\n",
       "        [[-3.14230114e-01, -1.09432541e-01,  8.89287055e-01],\n",
       "         [-2.12718439e+00, -7.06658065e-01,  5.93211986e-02],\n",
       "         [-6.09807074e-01,  1.42611778e+00,  6.50388300e-01],\n",
       "         [ 1.89216638e+00, -1.23307145e+00,  8.33525509e-02],\n",
       "         [ 1.09964669e+00,  2.50565529e-01,  6.64769113e-01],\n",
       "         [ 2.28642511e+00, -2.60160744e-01, -1.93993473e+00],\n",
       "         [-7.39896953e-01,  1.57108641e+00,  1.80546954e-01],\n",
       "         [-2.75018692e+00,  2.13222218e+00, -1.21088576e+00]],\n",
       "\n",
       "        [[ 9.62874711e-01, -1.60388255e+00, -1.20409369e+00],\n",
       "         [-6.83446109e-01, -2.26406500e-01, -1.53569889e+00],\n",
       "         [-3.54998112e-01, -1.13824463e+00, -2.11670375e+00],\n",
       "         [ 5.39169848e-01, -1.21673763e+00,  6.78258240e-02],\n",
       "         [-1.80813217e+00,  2.67976020e-02, -7.29233444e-01],\n",
       "         [-2.04087949e+00,  6.89116269e-02,  1.67060423e+00],\n",
       "         [ 1.09263813e+00,  6.48028374e-01, -1.72845638e+00],\n",
       "         [ 1.23426878e+00, -1.08520854e+00,  4.69182074e-01]],\n",
       "\n",
       "        [[-3.58166993e-02,  1.45487618e+00,  1.05393171e+00],\n",
       "         [ 1.25073934e+00, -1.34706748e+00, -5.24031878e-01],\n",
       "         [-6.46489739e-01,  1.41660905e+00, -3.17970991e-01],\n",
       "         [ 1.64032832e-01,  4.34944063e-01,  1.08385849e+00],\n",
       "         [-3.63862365e-01,  6.51489854e-01,  7.60915339e-01],\n",
       "         [ 1.17001021e+00, -9.54703033e-01, -1.07761323e+00],\n",
       "         [-8.62295777e-02,  5.33508182e-01,  1.57806575e-01],\n",
       "         [ 1.83115983e+00, -2.31795497e-02, -1.14060827e-01]],\n",
       "\n",
       "        [[-5.11722744e-01, -7.03672111e-01,  5.01701653e-01],\n",
       "         [-1.53182596e-01,  4.40406412e-01, -4.66138572e-01],\n",
       "         [ 1.65729761e+00, -2.42377710e+00, -9.30976775e-03],\n",
       "         [ 1.22728086e+00,  1.63046300e-01, -1.70604622e+00],\n",
       "         [ 5.80382109e-01, -1.45451665e+00,  1.46291137e-01],\n",
       "         [-8.59528184e-02, -1.02007318e+00,  3.65170658e-01],\n",
       "         [ 1.48979974e+00, -2.38096789e-01, -4.60102379e-01],\n",
       "         [-9.53995585e-01,  1.42034996e+00,  5.08343756e-01]],\n",
       "\n",
       "        [[ 2.41718936e+00,  2.52500623e-01, -7.11339116e-01],\n",
       "         [ 1.42091036e+00,  1.30928469e+00, -4.01691526e-01],\n",
       "         [ 1.76720405e+00, -1.20562351e+00, -1.62171558e-01],\n",
       "         [-5.45190036e-01, -1.56589642e-01, -2.16518030e-01],\n",
       "         [-3.00890326e+00,  3.75019461e-01, -1.03204048e+00],\n",
       "         [-1.03771195e-01, -8.21792245e-01, -5.65176427e-01],\n",
       "         [ 2.49378793e-02, -8.66699040e-01, -2.00387239e+00],\n",
       "         [-2.15074554e-01, -6.36201322e-01, -1.29887879e+00]],\n",
       "\n",
       "        [[ 7.40026951e-01,  1.00762880e+00, -1.56568270e-03],\n",
       "         [ 3.98409307e-01, -5.57215631e-01, -1.56158543e+00],\n",
       "         [-1.32287884e+00, -2.45854154e-01,  8.00924420e-01],\n",
       "         [-7.72385359e-01,  2.80509973e+00,  1.17510068e+00],\n",
       "         [-1.37861931e+00, -9.47751999e-01,  1.57342303e+00],\n",
       "         [-7.04157799e-02, -1.35377371e+00,  2.29580805e-01],\n",
       "         [ 5.79243243e-01,  2.41659865e-01,  2.16853333e+00],\n",
       "         [ 4.42102253e-01,  1.13662148e+00,  1.57549250e+00]],\n",
       "\n",
       "        [[ 2.49844462e-01, -1.56340027e+00,  2.84965485e-01],\n",
       "         [ 4.28009182e-01, -3.84085029e-01, -9.72273469e-01],\n",
       "         [ 1.28223526e+00,  8.92749906e-01, -7.37732232e-01],\n",
       "         [ 5.14733136e-01, -4.07288373e-01, -2.71693587e-01],\n",
       "         [ 2.39725128e-01,  9.62545097e-01, -5.08328795e-01],\n",
       "         [ 2.17626497e-01,  4.50851887e-01, -4.94630218e-01],\n",
       "         [ 1.18916690e-01,  8.00078809e-01, -6.94616914e-01],\n",
       "         [-5.55177331e-01,  6.80705830e-02,  1.94014892e-01]]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_caches['A_prev1 conv2d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in grads: ['dW1 dense', 'db1 dense', 'dA2 flatten', 'dA1 PoolLayer', 'dZ1 ReLu', 'dW1 conv2d', 'db1 conv2d', 'dA2 PoolLayer', 'dZ2 ReLu', 'dW2 conv2d', 'db2 conv2d']\n"
     ]
    }
   ],
   "source": [
    "grads = conv_net_backward(model , A_out , y_batch , 0.001)\n",
    "print(\"Keys in grads:\", list(grads.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.04701317, -0.06372207,  0.05919323,  0.04804708,\n",
       "          -0.03173059,  0.0477537 ,  0.04008454,  0.07483662],\n",
       "         [ 0.0879787 ,  0.01252406,  0.00900483, -0.0046065 ,\n",
       "           0.05733547,  0.01931735, -0.03816056, -0.14284468],\n",
       "         [-0.0024332 ,  0.00409315, -0.02246164, -0.02046628,\n",
       "          -0.00387669, -0.04575265,  0.02427689, -0.06683799]],\n",
       "\n",
       "        [[ 0.03052255,  0.0315146 , -0.07496332,  0.09032764,\n",
       "           0.03411074, -0.02634919,  0.05656244,  0.03023276],\n",
       "         [-0.02191041, -0.02229101,  0.07038713,  0.06335997,\n",
       "          -0.04141929, -0.00505811,  0.00635207, -0.00802328],\n",
       "         [-0.00126535, -0.0255095 ,  0.08382687,  0.03290524,\n",
       "           0.03654223, -0.07992808, -0.02312746, -0.10790746]],\n",
       "\n",
       "        [[-0.02733469,  0.04457072, -0.05207625, -0.03088645,\n",
       "           0.06113013, -0.03147708,  0.00330668, -0.05127046],\n",
       "         [ 0.02326311, -0.02718658, -0.09092722, -0.05660406,\n",
       "          -0.04673655,  0.00944437,  0.07034207, -0.05110324],\n",
       "         [-0.05336585,  0.03026428,  0.07442176,  0.00641954,\n",
       "           0.00390493,  0.01874174,  0.03693086,  0.00295826]]],\n",
       "\n",
       "\n",
       "       [[[-0.09901625,  0.01173062, -0.00951773,  0.02793358,\n",
       "           0.05029225,  0.022296  ,  0.08335347,  0.03550253],\n",
       "         [ 0.04508889, -0.09560424, -0.03692587, -0.02519139,\n",
       "           0.00917426,  0.04547485,  0.02258523, -0.07090108],\n",
       "         [ 0.0560138 , -0.06409779, -0.00858516, -0.00644132,\n",
       "           0.00520642,  0.01646762, -0.02790846,  0.00417246]],\n",
       "\n",
       "        [[-0.03171075,  0.02261857, -0.09172018, -0.07139531,\n",
       "          -0.01189273,  0.09906834,  0.03073375, -0.05039373],\n",
       "         [-0.03567313, -0.01773588,  0.03903587,  0.03264534,\n",
       "          -0.01072986, -0.02673906,  0.00812329, -0.04659012],\n",
       "         [-0.00078008, -0.01945004, -0.05446974,  0.02611045,\n",
       "          -0.00472968,  0.01854689,  0.04667701, -0.03114722]],\n",
       "\n",
       "        [[ 0.04167856, -0.07148145,  0.00535192,  0.006802  ,\n",
       "           0.02081321,  0.03816603,  0.04335532, -0.09140035],\n",
       "         [-0.02488386, -0.04173281, -0.01899233, -0.02953221,\n",
       "           0.02320325,  0.04299324, -0.02529829,  0.06641315],\n",
       "         [ 0.0224009 , -0.03673758,  0.02841949,  0.02035767,\n",
       "           0.0260799 , -0.01478296, -0.0441467 ,  0.07217481]]],\n",
       "\n",
       "\n",
       "       [[[ 0.01426599, -0.05200345, -0.00811415,  0.12096325,\n",
       "          -0.00765088,  0.03052134, -0.07871035,  0.01030001],\n",
       "         [ 0.01931661,  0.00377019,  0.09733691, -0.0564675 ,\n",
       "           0.05936471, -0.01782685,  0.00277552, -0.03942178],\n",
       "         [-0.03288579, -0.01559419,  0.03483442, -0.04529634,\n",
       "          -0.05882316, -0.03059804, -0.08881418, -0.04136458]],\n",
       "\n",
       "        [[ 0.03537426,  0.0053993 , -0.0365847 ,  0.01158266,\n",
       "          -0.0006035 ,  0.02632253, -0.02172068, -0.05072855],\n",
       "         [-0.12210038,  0.03106326,  0.01902339, -0.01915271,\n",
       "          -0.03676012, -0.03212763, -0.08614253,  0.07919168],\n",
       "         [ 0.00070046,  0.00217876, -0.03485209, -0.08895786,\n",
       "           0.03673037, -0.044961  , -0.04288471,  0.0334529 ]],\n",
       "\n",
       "        [[ 0.08679569, -0.03921782, -0.02476712,  0.00571799,\n",
       "          -0.01243345,  0.01375197, -0.03003838, -0.03786046],\n",
       "         [ 0.00543513,  0.03510212, -0.11115355,  0.00599047,\n",
       "           0.0775836 ,  0.04632122,  0.00271771, -0.09056904],\n",
       "         [-0.09433677,  0.06010768,  0.02237377, -0.05453974,\n",
       "          -0.04598428, -0.01223092, -0.02780239,  0.00483649]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads['dW2 conv2d']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trian the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [X_batch.shape[3] , 8 , 16]\n",
    "pool = [1 , 1 ,1]\n",
    "batch_size = 16\n",
    "lr = 0.001\n",
    "\n",
    "model, history = full_cnn(filters , pool , train_df , 5 , batch_size , lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pablo Reyes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
